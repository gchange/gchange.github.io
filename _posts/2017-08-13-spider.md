---
layout: default
title:  爬虫篇之基础入门
---

<h1>{{ page.title }}</h1>

[github](https://github.com/gchange/spider)

## 爬取网站的3种常见方法
* 爬取网站地图
* 遍历每个页面的数据库ID
* 跟踪网页链接

### 网站地图
方便搜索引擎机器人找到网站中的页面而编写的描述了网站架构的文档。
* [网站地图](https://zh.wikipedia.org/wiki/%E7%B6%B2%E7%AB%99%E5%9C%B0%E5%9C%96)
* [sitemap语法介绍](http://wiki.jikexueyuan.com/project/seo/24.html)

### 遍历每个页面的数据库ID
这只适用与特定结构的网站，限制条件较多。

### 跟踪网页链接
由一个跟页面进入，逐步深入到其他链接，这种方式更像一个普通用户的行为。

## 高级功能

### 解析robots.txt
为了避免下载禁止爬取的URL，爬取前需先解析robots.txt

### 支持代理
有些网站会屏蔽某些IP进行访问或者会会同一个IP获取做限制。

### 下载限速
如果我们爬取网站的速度过快，就会面临被封禁或是造成服务器过载的风险。为了降低这些风险，我们可以在两次下载之间添加延时，从而对爬虫限速。

### 爬虫陷阱
一些网站会动态生成页面内容，这样会出现无限多的链接，这种情况称为爬虫陷阱；为了避免进入爬虫陷阱，一个简单的方法是记录当前网页的深度，爬虫之会爬取一定深度以内的页面。

#### 参考：
* [用Python写网络爬虫](http://www.epubit.com.cn/book/onlinechapter/42493)
* [网站地图](https://zh.wikipedia.org/wiki/%E7%B6%B2%E7%AB%99%E5%9C%B0%E5%9C%96)
* [sitemap语法介绍](http://wiki.jikexueyuan.com/project/seo/24.html)

<p>{{ page.date | date_to_string }}</p>
